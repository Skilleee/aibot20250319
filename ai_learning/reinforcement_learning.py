import logging

import gym
import numpy as np
import pandas as pd
from gym import spaces
from stable_baselines3 import PPO

# Konfigurera loggning
logging.basicConfig(filename="reinforcement_learning.log", level=logging.INFO)


class TradingEnv(gym.Env):
    """
    En ut칬kad f칬rst칛rkningsinl칛rningsmilj칬 f칬r trading, baserad p친
    ditt tidigare exempel. Den hanterar nu en 'holding' (antal enheter)
    och en 'balance' (t.ex. USD). Action = 0=HOLD, 1=BUY, 2=SELL.
    Reward = f칬r칛ndring i portf칬ljv칛rde (balance + holding*price).
    """

    def __init__(self, data, initial_balance=10_000):
        """
        data: Pandas DataFrame med kolumner som ["close", "momentum", "volume", "return"].
        initial_balance: Startkapital i t.ex. USD.
        """
        super(TradingEnv, self).__init__()

        self.data = data.reset_index(drop=True)
        self.current_step = 0
        self.initial_balance = initial_balance

        # Action space: 0=HOLD, 1=BUY, 2=SELL
        self.action_space = spaces.Discrete(3)

        # Exempel: observation med [pris, momentum, volume, balance, holding]
        # Du kan l칛gga till "return" eller ta bort "volume" etc. efter behov.
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32
        )

        # Interna variabler (balance, holding, etc.)
        self.reset()

    def _get_obs(self):
        """
        H칛mtar observationen: [close, momentum, volume, balance, holding].
        """
        row = self.data.iloc[self.current_step]
        close = row["close"]
        momentum = row["momentum"]
        volume = row["volume"]

        obs = np.array([
            close,
            momentum,
            volume,
            self.balance,
            self.holding
        ], dtype=np.float32)

        return obs

    def reset(self):
        """
        칀terst칛ll milj칬n till startl칛ge.
        """
        self.current_step = 0
        self.balance = float(self.initial_balance)
        self.holding = 0.0  # hur m친nga "units" vi 칛ger av instrumentet
        return self._get_obs()

    def step(self, action):
        """
        Utf칬r en handling (0=HOLD, 1=BUY, 2=SELL).
        Returnerar (obs, reward, done, info).
        """
        # Spara gamla portf칬ljv칛rdet f칬r reward-ber칛kning
        old_value = self._get_portfolio_value()

        # G칬r action
        self._take_action(action)

        self.current_step += 1
        done = (self.current_step >= len(self.data) - 1)

        # R칛kna ut reward som skillnad i portf칬ljv칛rde
        new_value = self._get_portfolio_value()
        reward = new_value - old_value

        obs = self._get_obs() if not done else np.zeros(self.observation_space.shape)
        info = {}

        return obs, reward, done, info

    def _take_action(self, action):
        """
        En f칬renklad k칬p/s칛lj-logik.
        - BUY = investera 10% av balance
        - SELL = s칛lj allt vi 칛ger
        - HOLD = g칬r ingenting
        """
        # H칛mta priset p친 nuvarande step
        price = self.data.iloc[self.current_step]["close"]

        if action == 1:  # BUY
            amount_to_spend = 0.1 * self.balance
            units = amount_to_spend / price
            self.holding += units
            self.balance -= amount_to_spend

        elif action == 2:  # SELL
            # s칛lj all holding
            self.balance += self.holding * price
            self.holding = 0.0

        # HOLD => g칬r ingenting

    def _get_portfolio_value(self):
        """
        Nuvarande portf칬ljv칛rde = balance + holding * price
        """
        price = self.data.iloc[self.current_step]["close"]
        return self.balance + self.holding * price


if __name__ == "__main__":
    # Exempel: Generera simulerad data
    np.random.seed(42)
    data = pd.DataFrame(
        {
            "close": np.cumsum(np.random.randn(1000) * 2 + 100),
            "momentum": np.random.randn(1000),
            "volume": np.random.randint(100, 1000, size=1000),
            "return": np.random.randn(1000) / 100,
        }
    )

    env = TradingEnv(data, initial_balance=10_000)

    # Tr칛na en PPO-modell
    model = PPO("MlpPolicy", env, verbose=1)
    model.learn(total_timesteps=10_000)

    model.save("rl_trading_model.zip")
    print("游닉 RL-modell 칛r tr칛nad och sparad!")
