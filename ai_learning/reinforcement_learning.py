import logging
import gym
import numpy as np
import pandas as pd
from gym import spaces
from stable_baselines3 import PPO

# Konfigurera loggning
logging.basicConfig(
    filename="reinforcement_learning.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

class TradingEnv(gym.Env):
    """
    En f칬rb칛ttrad f칬rst칛rkningsinl칛rningsmilj칬 f칬r trading.
    
    Denna milj칬 hanterar en portf칬lj med ett initialt kapital (balance) och
    en m칛ngd innehav (holding) av en tillg친ng. Handlingsutrymmet:
        0: HOLD (inga f칬r칛ndringar)
        1: BUY (k칬p med 10% av aktuell balans)
        2: SELL (s칛lj alla innehav)
    
    Observationen 칛r en vektor: [close, momentum, volume, balance, holding].
    Bel칬ningen definieras som f칬r칛ndringen i portf칬ljv칛rde.
    """
    metadata = {'render.modes': ['human']}

    def __init__(self, data: pd.DataFrame, initial_balance: float = 10_000):
        """
        Initierar milj칬n med historiska data och startkapital.
        
        Args:
            data (pd.DataFrame): DataFrame med kolumnerna ["close", "momentum", "volume", "return"].
            initial_balance (float): Startkapital, t.ex. USD.
        """
        super(TradingEnv, self).__init__()
        self.data = data.reset_index(drop=True)
        self.initial_balance = initial_balance

        # Definiera action space: 0=HOLD, 1=BUY, 2=SELL
        self.action_space = spaces.Discrete(3)
        # Definiera observation space: [close, momentum, volume, balance, holding]
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32)

        # Interna tillst친nd
        self.current_step = 0
        self.balance = float(self.initial_balance)
        self.holding = 0.0

        # F칬r reproducibilitet
        self.seed()

    def seed(self, seed=None):
        """
        S칛tter seed f칬r slumpgenereringen.
        
        Returns:
            list: Seed som anv칛nds.
        """
        self.np_random, seed = gym.utils.seeding.np_random(seed)
        return [seed]

    def _get_obs(self) -> np.ndarray:
        """
        H칛mtar observationen: [close, momentum, volume, balance, holding].
        
        Returns:
            np.ndarray: Nuvarande observation.
        """
        row = self.data.iloc[self.current_step]
        obs = np.array([
            row["close"],
            row["momentum"],
            row["volume"],
            self.balance,
            self.holding
        ], dtype=np.float32)
        return obs

    def reset(self) -> np.ndarray:
        """
        칀terst칛ller milj칬n till startl칛ge och returnerar den initiala observationen.
        
        Returns:
            np.ndarray: Initial observation.
        """
        self.current_step = 0
        self.balance = float(self.initial_balance)
        self.holding = 0.0
        logging.info("Milj칬n 친terst칛lld: balance=%.2f, holding=%.2f", self.balance, self.holding)
        return self._get_obs()

    def step(self, action: int):
        """
        Utf칬r en given handling och returnerar (observation, reward, done, info).
        
        Args:
            action (int): 0 (HOLD), 1 (BUY) eller 2 (SELL).
            
        Returns:
            tuple: (obs, reward, done, info)
        """
        # Spara nuvarande portf칬ljv칛rde f칬r rewardber칛kning
        old_value = self._get_portfolio_value()

        # Utf칬r handling
        self._take_action(action)
        logging.debug("Utf칬rd handling: %d vid steg %d", action, self.current_step)

        # G친 till n칛sta steg
        self.current_step += 1
        done = self.current_step >= len(self.data) - 1

        # Ber칛kna reward baserat p친 f칬r칛ndring i portf칬ljv칛rde
        new_value = self._get_portfolio_value()
        reward = new_value - old_value
        logging.info("Steg: %d, Action: %d, Reward: %.2f, Portf칬ljv칛rde: %.2f",
                     self.current_step, action, reward, new_value)

        # H칛mta n칛sta observation
        obs = self._get_obs() if not done else np.zeros(self.observation_space.shape, dtype=np.float32)
        info = {}
        return obs, reward, done, info

    def _take_action(self, action: int):
        """
        Utf칬r en f칬renklad k칬p/s칛lj-logik.
        
        BUY: Investera 10% av nuvarande balans.
        SELL: S칛lj alla innehav.
        HOLD: G칬r ingenting.
        """
        price = self.data.iloc[self.current_step]["close"]
        if action == 1:  # BUY
            amount_to_spend = 0.1 * self.balance
            if amount_to_spend > 0:
                units = amount_to_spend / price
                self.holding += units
                self.balance -= amount_to_spend
                logging.debug("K칬pte: %.4f enheter till pris %.2f", units, price)
        elif action == 2:  # SELL
            if self.holding > 0:
                self.balance += self.holding * price
                logging.debug("S친lde: %.4f enheter till pris %.2f", self.holding, price)
                self.holding = 0.0
        # Om action == 0 (HOLD) g칬rs inget

    def _get_portfolio_value(self) -> float:
        """
        Ber칛knar nuvarande portf칬ljv칛rde: balance + holding * current price.
        
        Returns:
            float: Nuvarande portf칬ljv칛rde.
        """
        price = self.data.iloc[self.current_step]["close"]
        return self.balance + self.holding * price

    def render(self, mode="human"):
        """
        Render-funktion f칬r att visa aktuell status f칬r portf칬ljen.
        """
        portfolio_value = self._get_portfolio_value()
        print(f"Steg: {self.current_step} | Balance: {self.balance:.2f} | Holding: {self.holding:.4f} | Portf칬ljv칛rde: {portfolio_value:.2f}")

    def close(self):
        """
        St칛nger milj칬n och frig칬r resurser vid behov.
        """
        logging.info("Milj칬n st칛ngs.")

if __name__ == "__main__":
    # Exempel: Generera simulerad data
    np.random.seed(42)
    data = pd.DataFrame({
        "close": np.cumsum(np.random.randn(1000) * 2 + 100),
        "momentum": np.random.randn(1000),
        "volume": np.random.randint(100, 1000, size=1000),
        "return": np.random.randn(1000) / 100,
    })

    env = TradingEnv(data, initial_balance=10_000)

    # Tr칛na en PPO-modell med Stable Baselines3
    model = PPO("MlpPolicy", env, verbose=1)
    model.learn(total_timesteps=10_000)
    model.save("rl_trading_model.zip")
    print("游닉 RL-modell 칛r tr칛nad och sparad!")
